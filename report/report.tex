\documentclass[a4paper,12pt]{article}

\usepackage{url}
\usepackage{epsfig}
\usepackage{graphics}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{pdflscape}
\usepackage{longtable}
\usepackage[noend, linesnumbered]{algorithm2e}


\graphicspath{{pictures/}}

\title{Detecting context-dependent word defects in written English by native Swedes with n-grams}
\author{\hspace*{-0.5cm}\begin{tabular}{cccc}
Frej Connolly & Maja Gidlund & Sandra Liljeqvist & Sara Norrby \\
connolly@kth.se & majagi@kth.se & sanlil@kth.se & saranorr@kth.se \\
\includegraphics[width=0.13\linewidth]{frej} & 
\includegraphics[width=0.13\linewidth]{maja} & 
\includegraphics[width=0.13\linewidth]{sandra} & 
\includegraphics[width=0.13\linewidth]{sara}
\end{tabular}}
\date{2014-10-05}

\pagestyle{fancy}
\setlength{\headheight}{15pt}
\fancyhf{}
\lhead{DD2380 ai14} % DO NOT REMOVE!!!!
\rhead{F. Connolly, M. Gidlund, S. Liljeqvist, S. Norrby}

\begin{document}

\maketitle
\thispagestyle{fancy}

\begin{abstract}
TODO
\end{abstract}



\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:introduction}
The need to be able to write accurate English is growing as the market is becoming more global. For native Swedish speakers it can be easy to write a word that does not make sense in a certain context, by mistake. A traditional spell checker will not detect this kind of mistake since the words themselves often are correct, but placed in the wrong context.

For example if a Swede wants to ask a friend if he has a dinner jacket, the person might say \emph{Do you have a smoking?} believing that the word smoking (Swedish word for dinner jacket) has the same meaning in English. The native English speaker could interpret that what the Swede really intended to say was \emph{Do you have a cigarette?}  

A normal spell checker would not be able to find an error like this, but an algorithm that looks at the probability of the word order might. The standard way to implement this type of word prediction is by using the so called n-gram model. The n-gram model is based on 
statistical language modeling and uses the $n-1$ previous words in a sentence to predict the next word. Examples of n-grams is unigram $n=1$, bigrams $n=2$ and trigrams $n=3$. This is covered in more detail later on in the report section \ref{sec:ngram} \cite{gallagher2004natural}.

We anticipate that a tool that could locate this kind of mistake could be of great help. As far as we know there is no tool like this specialized in Swedes writing in English. The tool should work as a context-dependent word detection.

\subsection{Objective}
\label{sec:objective}
The aim for this project is to investigate if it is possible to implement a system that will find words that are out of context by using n-grams. We will implement a program and analyze if it can identify this kind of words in different sentences. The application area for this system will be for people with Swedish as a first language and English as a foreign language, who wants to write in English without making unnecessary mistakes.

\subsection{Hypothesis}
Our hypothesis is that it is enough to use bigrams and trigrams and a probability threshold to detect words that are out of context in a given text. However, the results will be affected by the quality of the corpus (covered in \ref{sec:ngram}) and we believe that a large and well chosen corpus is vital for the result of this study.

We believe that a combination of bigrams and trigrams will make a good balance in the algorithm and that it will suffice to reach the goal (covered in \ref{sec:method}). The reason for that is partly that the two methods would complement each other well with different but necessary information. As the investigation focuses on the context, the use of unigrams would not be relevant, since it is only based on the current word. On the other hand, having the same four (or more) words in a row is very rare, therefore we decided to not use n-grams that are bigger than three, as the amount of data would not be enough to make informed decisions.

\subsection{Contribution}
\label{sec:contribution}
The purpose of our program is to reduce the number of incorrect uses of  words in writing. False friends and pseudo-anglicism are strongly related to the problem of expressing ourselves improperly in a language other than our mother tongue. In these cases a spell checker would not react since the words are correctly spelled, but used in the wrong context. Since millions of people speak English as a foreign language and many of the languages, that these people has as their first language, have false friends with English. This could help these people to better understand and to learn the English language. 

Previous work has been done in this field for several different languages, but in our work we will focus on the Swedish languages and the difficulties people with Swedish as their first language have with English.

\subsection{Outline}
\label{sec:outline}
TODO

\section{Language Modeling}
\subsection{N-gram modeling}
\label{sec:ngram}
In general a statistical language modeling is based on n-gram language modeling which tries to predict the next word in a sentence based on the $n-1$previous words in the sentence \cite{gallagher2004natural}.  When choosing two words to compute the probability that a given word is the next word is called bigram. If we for example have the sentence \emph{I painted the house in the color} and uses bigram to guess the next word in the sentence we would compute $\Pr(predictive word \mid color)$. Although it makes more sense to assume, by using the whole sentence, that it would be easier for a human to guess the next word. This because the guess then could also be based on the context of the sentence. However, the bigram does not take as much computer power unlike if one wants to compute the whole sentence. It also takes less time to compute a bigram in comparison to compute a whole sentence \cite{gallagher2004natural}.

The information that is needed to compute an n-gram is how often a word occurs in general and how often a word occurs with others specific words. The n-gram model is using the Markov assumption, which is the assumption that the future behavior of a dynamical system only depends on its recent history \cite{mooneynatural}. In this case the history of the $n-1$ previous words. 

The n-gram model is often used together with one or several corpora. This is a large collection of text, written by humans, for different purposes that can be used for analyzing natural language. A great volume of text like this is necessary to make a probabilistic model, like n-grams, effective, since this approach requires that the system is learning from a big amount of data \cite{russell2010ai}.

\subsection{Problem with sparse data}
The process of smoothing is used to improve the reliability of the n-gram frequency, which occurs because of the low frequency many n-grams tend to have. What smoothing does is that it is reducing the numbers of zeros of probabilities. This means that if a system uses trigram, the weighted probability is the sum of the trigram, the bigram and the unigram. Therefore, if a system wants to combine different methods to predict the next word it could still be predicted right even though its probability of the trigram tends to be extremely low \cite{smoothing}.

\subsection{False Friends or Cognates}
In the paper Automatic Identification of Cognates and False Friends in French and English the authors Diana Inkpen, Oana Frunza and Grzegorz Kondrak bring up false friends and cognates \cite{frunzaInkpen2009identification}. In most languages there are words that are similar, in spelling, to words in other languages. Sometimes this is helpful, cognates are pairs of words in two different languages that have similar spelling and also the same meaning. One example of a pair, mentioned by Inkpen, Frunza and Kondrak, is the French nature and the English nature. They are spelled exactly the same and have the same meaning.

But sometimes the use of words that are similar is a bad idea. False friends are pairs of words that look alike, but mean completely different things. One example of this is french main, which means hand, and main in English. There are also some words that in some contexts have the same meaning but in others have a completely different one \cite{frunzaInkpen2009identification}.

\subsection{Pseudo-anglicisms}
Pseudo-anglicisms are words, that a language has borrowed from the English language, that do not make sense in English. These kinds of words are related to false friends but is not entirely the same. One example in German is body-bag which is a kind of backpack with only one strap. Another example is the Swedish word backslick, which in English really is called slick-back hairdo. Sometimes the words do not mean anything at all in English but sometimes the words have a completely different meaning \cite{pseudoanglicism}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related work}
\label{sec:relatedwork}
The paper Techniques for automatically correcting words in text by Karen Kukich brings up three different types of error detection and correction. The type most interesting to us was context dependent word-correction. Kukich discusses previous work and different approaches for the context dependent word-correction \cite{Kukich1992Tecniques}. However, in this report the focus is not on word-correction but on word-detection.

Several research papers have discussed how to capture the context of a sentence without having to analyze the whole sentence. In the article A Swedish Grammar for Word Prediction, written by Ebba Gustavii and Eva Pettersson, grammar rules was implemented by using the FASTY language modeling to predict the next word with the use of the n-gram modeling \cite{gustavii2003a}. The article's aim was to cover contexts where statistical models gives irrelevant suggestions. The hypothesis of the article was that the FASTY would achieve a good and intelligent choice of words. The particularly useful part of this article was the method for measuring and evaluating the results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Method}
\label{sec:method}
The project is divided into two parts: implementation and analyzing.

\subsection{Implementation}
\label{ref:implementation}
The implementation is based on the previously mentioned n-gram model. Based on the hypothesis, bigrams was used to be able to find plausible incorrect words. The text processing library Natural Language Toolkit (NLTK) in Python is conveniently used to save time for already solved problems. It is a platform for language modeling and it provides many corpora, lexical resources and provides common NLP functions\cite{nltk}. 

To create a good library of bigrams  two corpora and a list of most frequently bigrams were used.  The Brown corpus contains 1,15 million words which is gathered from works published in the United States in 1964\cite{francis64brown}. The web text corpus used contains 200 000 words text with less formal language with content extracted from discussions, movie scripts, wine reviews etc. \cite{nltkWebtext}. The data set was further improved by using the 286 000 most frequently used bigrams from websites collected by Google in \emph{Google Web Trillion Word Corpus} \cite{286}.  This resulted in a total set of bigrams of 627 353. 

\subsection{Analyzing}
\label{ref:analyzing}
A bigram was considered to be proper English if it was included in the constructed bigram library, and incorrect if it was not. A more advanced statistical method could not be used since it would require a larger corpus. 

For measuring the results we calculated the quota between the correctly detected defects and all defects together with incorrectly detected defects. This equation is called \emph{hit rate} \cite{ghayoomi2005word}.

$$z = \frac{\sum correct}{\sum correct + \sum not\_detected + \sum incorrect}$$
\begin{description}
  \item[correct] \hfill \\
  		correctly detected defects
  \item[not\_detected] \hfill \\
  		defects not detected
  \item[incorrect] \hfill \\
   		incorrect defects detected
\end{description}

If $z$ equals one, the algorithm correctly found all defects with no incorrect matches and if z is zero, it did not found any of the defects.

For the experiment we used both incorrect sentences and the corresponding correct version of each sentence. The sentences were constructed as simple sentences with one incorrect word in each one. The incorrect words were false friends, real-word errors or grammatical errors. In the correct sentences, these incorrect words, were replaced with the word that made sense in the sentence. 

For the hit rate calculation of the incorrect sentences, the true positives are the correctly detected defects, the negative positives are the defects that are not detected and false positives are the incorrect defects detected words \ref{alg:incorrect_sentence}. In a similar way, the true positives, for the correct sentences, are the total number of bigrams except the found defects, zero false negatives and false positives the number of found defects \ref{alg:correct_sentence}.
 
\label{alg:incorrect_sentence}
\begin{figure}[h]
\begin{algorithm}[H]
    \DontPrintSemicolon
    \SetKwFunction{KwFn}{hitrateincorrect}
    \SetAlgoLined
    \renewcommand\bottomfraction{0.85}
    \KwData{sentence = text to measure containing exactly one wrong word,
				  wrong\_word = word in sentence that does not make sense,
            	  library = set of bigrams constructed from corpus}
    \KwResult{Hit rate for sentence $n$}
    \SetKwFunction{hitrateincorrect}{hitrate\_incorrect\_sentence}
    \SetKwProg{myhitrateincorrect}{function}{}{}
    \myhitrateincorrect{\hitrateincorrect{sentence, wrong\_word, library}}{
		$bigrams \leftarrow$ construct\_bigrams(sentence)\;
		$correct\_defects \leftarrow$ bigrams containing wrong\_word\;		
		$found\_defects \leftarrow$ bigrams not in library\;
		$incorrect \leftarrow$ count(found\_defects not in correct\_defects)\;
		\If{any correct\_defects in found\_defects}{
			$correct \leftarrow 1$\;
			$not\_detected \leftarrow 0$\;
        }
        \Else{
			$correct \leftarrow 0$\;
			$not\_detected \leftarrow 1$\;
        }				
		\Return{correct / (correct + not\_detected + incorrect)}
    }
    \caption{Calculate hitrate for the sentences containing exactly one wrong word.}
\end{algorithm}
\end{figure}

\label{alg:correct_sentence}
\begin{figure}[h]
\begin{algorithm}[H]
    \DontPrintSemicolon
    \SetKwFunction{KwFn}{hitratecorrect}
    \SetAlgoLined
    \renewcommand\bottomfraction{0.85}
    \KwData{sentence = text to measure containing no errors
            	  library = set of bigrams constructed from corpus}
    \KwResult{Hit rate for sentence $n$}
    \SetKwFunction{hitratecorrect}{hitrate\_correct\_sentence}
    \SetKwProg{myhitratecorrect}{function}{}{}
    \myhitratecorrect{\hitratecorrect{sentence, library}}{
		$bigrams \leftarrow$ construct\_bigrams(sentence)\;
		$found\_defects \leftarrow$ bigrams not in library\;
		$incorrect \leftarrow$ count(found\_defects)\;
		$correct \leftarrow $ count(bigrams) - incorrect\;
		$not\_detected \leftarrow 0$\;
		\Return{correct / (correct + not\_detected + incorrect)}
    }
    \caption{Calculate hitrate for the sentences containing exactly one wrong word.}
\end{algorithm}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental results}
\label{sec:experimentalresults}

\begin{landscape}
\centering
\begin{longtable}{l l l r l}
Hitrate & Wrong word & Sentence & Count & Defects found \\
\hline
0,000 & smoking & Do you have a smoking & 0 &  \\
1,000 & pied & It has pied & 1 & (has, pied) \\
1,000 & overdrive & Do not overdrive & 1 & (not, overdrive) \\
1,000 & about & Have you changed about & 1 & (changed, about) \\
0,000 & smell & The door was closed with a smell & 0 &  \\
0,000 & shoot & Can you shoot the window & 0 &  \\
1,000 & strike & Do you want to strike your clothes & 1 & (strike, your) \\
1,000 & bear & I like to eat bear & 1 & (eat, bear) \\
0,000 & jump & I think I will jump over the tea this morning & 1 & (tea, this) \\
1,000 & spoiling & We had a problem spoiling the toilet & 2 & (problem, spoiling), (spoiling, the) \\
1,000 & rapefields & It is probably rapefields & 1 & (probably, rapefields) \\
1,000 & funny & I always have a funny time at parties & 1 & (funny, time) \\
1,000 & disabled & Disabled toilet & 1 & (disabled, toilet) \\
0,000 & are & When are you born & 1 & (you, born) \\
0,000 & diary & Sorry to hear about your diary & 0 &  \\
0,000 & kiss & I need to go the the toilet and kiss & 0 &  \\
1,000 & moose & Can you pass me the potato moose & 1 & (potato, moose) \\
1,000 & call & This morning was very call & 1 & (very, call) \\
0,000 & to & She is a friend to me & 0 &  \\
0,500 & swimming & Could I have some water because my friend is swimming & 2 & (water, because), (is, swimming) \\
0,000 & lend & I need to lend a pen from you & 1 & (pen, from) \\
1,000 & were & were are you & 1 & (were, are) \\
1,000 & learns & He learns children & 1 & (learns, children) \\
0,500 & controls & He controls passports at the airport & 3 & (he, controls), (controls, passports), (passports, at) \\
1,000 & semester & We are going on semester together & 2 & (on, semester), (semester, together) \\
1,000 & drunk & I am back drunk & 1 & (back, drunk) \\
0,500 & table & I like sandwich table & 2 & (like, sandwich), (sandwich, table) \\
1,000 & stark & She is very stark & 1 & (very, stark) \\
1,000 & flipper & I want to play flipper & 1 & (play, flipper) \\
1,000 & pockets & Have you read any good pockets lately & 2 & (good, pockets), (pockets, lately) \\
1,000 & vine & Can I have a glass of vine & 1 & (of, vine) \\
1,000 & basket & Do you want to play basket & 1 & (play, basket) \\
1,000 & backslick & I have my hair in a backslick & 1 & (a, backslick) \\
0,000 & receipt & Can I have the receipt for this soup & 1 & (this, soup) \\
0,000 & photograph & I work as a photograph & 0 &  \\
0,000 & fall & I am investigating this fall & 2 & (am, investigating), (investigating, this) \\
0,000 & rent & What is the rent on your bank loan & 0 &  \\
1,000 & vibrant & My phone is vibrant & 1 & (is, vibrant) \\
1,000 & glass & I like to eat glass & 1 & (eat, glass) \\
0,000 & brick & Put the cups on the brick & 1 & (cups, on) \\
1,000 & island & It is cold in island & 1 & (in, island) \\
1,000 & barns & These are my barns & 1 & (my, barns) \\
1,000 & expedition & I am going to the expedition on the fifth floor & 1 & (expedition, on) \\
0,000 & meaning & Can you write a meaning & 0 &  \\
1,000 & dans & May I have the pleasure of this dans & 1 & (this, dans) \\
0,000 & bio & Would you like to go to the bio & 0 &  \\
1,000 & slang & Use the water slang to water the flowers & 2 & (water, slang), (slang, to) \\
0,500 & bra & It is going to be bra weather tomorrow & 3 & (be, bra), (bra, weather), (weather, tomorrow) \\
0,333 & fond & I have started a fond for you college fees & 3 & (fond, for), (you, college), (college, fees) \\
0,000 & disk & I will do the disk & 0 &  \\
1,000 & mark & Dig a hole in this mark & 1 & (this, mark) \\
\hline
0,531 & total hitrate
% \caption{Result incorrect sentences.}  % Fick inte caption att funka
\label{tab:resultincorrect}
\end{longtable}
\end{landscape}

\begin{landscape}
\centering
\begin{longtable}{l l r l}
Hitrate & Sentence & Count & Defects found (should be none) \\
\hline
0,750 & Do you have a tuxedo & 1 & (a, tuxedo) \\
1,000 & It broke & 0 &  \\
0,333 & Do not overdo it & 2 & (not, overdo), (overdo, it) \\
0,666 & Have you changed outfit & 1 & (changed, outfit) \\
1,000 & The door was closed with a bang & 0 &  \\
1,000 & Can you shut the window & 0 &  \\
0,833 & Do you want to iron your clothes & 1 & (iron, your) \\
0,750 & I like to eat berries & 1 & (eat, berries) \\
0,750 & I think I will skip the tea this morning & 2 & (will, skip), (tea, this) \\
0,666 & We had a problem flushing the toilet & 2 & (problem, flushing), (flushing, the) \\
0,500 & It is probably rape fields & 2 & (probably, rape), (rape, fields) \\
1,000 & I always have a fun time at parties & 0 &  \\
0,750 & Toilet is out of order & 1 & (toilet, is) \\
0,333 & When were you born & 2 & (when, were), (you, born) \\
0,800 & Sorry to hear about your diarrhea & 1 & (your, diarrhea) \\
0,900 & I need to go the the toilet and have a pee & 1 & (a, pee) \\
0,833 & Can you pass me the mashed potatoes & 1 & (the, mashed) \\
1,000 & This morning was very cold & 0 &  \\
1,000 & She is a friend of mine & 0 &  \\
0,777 & Could I have some water because my friend is fainting & 2 & (water, because), (is, fainting) \\
0,857 & I need to borrow a pen from you & 1 & (pen, from) \\
1,000 & where are you & 0 &  \\
0,500 & He teaches children & 1 & (teaches, children) \\
0,400 & He checks passports at the airport & 3 & (he, checks), (checks, passports), (passports, at) \\
0,800 & We are going on holiday together & 1 & (holiday, together) \\
0,666 & I am hung over & 1 & (am, hung) \\
0,500 & I like smorgasbords & 1 & (like, smorgasbords) \\
1,000 & She is very strong & 0 &  \\
0,750 & I want to play pinball & 1 & (play, pinball) \\
0,666 & Have you read any good paperbacks lately & 2 & (good, paperbacks), (paperbacks, lately) \\
1,000 & Can I have a glass of wine & 0 &  \\
1,000 & Do you want to play basketball & 0 &  \\
0,833 & I have my hair in a slickback & 1 & (a, slickback) \\
0,857 & Can I have the recipe for this soup & 1 & (this, soup) \\
1,000 & I work as a photographer & 0 &  \\
0,500 & I am investigating this case & 2 & (am, investigating), (investigating, this) \\
1,000 & What is the interest on your bank loan & 0 &  \\
0,666 & My phone is vibrating & 1 & (is, vibrating) \\
0,800 & I like to eat ice cream & 1 & (eat, ice) \\
0,800 & Put the cups on the tray & 1 & (cups, on) \\
0,750 & It is cold in Iceland & 1 & (in, iceland) \\
1,000 & These are my children & 0 &  \\
1,000 & I am going to the office on the fifth floor & 0 &  \\
1,000 & Can you write a sentence & 0 &  \\
0,857 & May I have the pleasure of this dance & 1 & (this, dance) \\
1,000 & Would you like to go to the cinema & 0 &  \\
0,714 & Use the water hose to water the flowers & 2 & (water, hose), (hose, to) \\
0,857 & It is going to be good weather tomorrow & 1 & (weather, tomorrow) \\
0,750 & I have started a fund for you college fees & 2 & (you, college), (college, fees) \\
1,000 & I will do the dishes & 0 &  \\
1,000 & Dig a hole in this ground & 0 &  \\
\hline
0,825 & hitrate
% \caption{Result correct sentences.}  % Fick inte caption att funka
\label{tab:resultcorrect}
\end{longtable}
\end{landscape}

\subsection{Experimental setup}
TODO

\subsection{Experiment}
TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary and Conclusions}
\label{sec:summary}
TODO


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{plain}
\bibliography{reflist}
\end{document}
